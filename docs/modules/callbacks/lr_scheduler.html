

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>callbacks.lr_scheduler &mdash; torchtools 0.1.3 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> torchtools
          

          
          </a>

          
            
            
              <div class="version">
                0.1.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Welcome to torchtoolsâ€™s documentation!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../home.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trainer.html">Trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../meters.html">Meters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../exceptions.html">Exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../acknoledgement.html">Acknoledgement</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">torchtools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>callbacks.lr_scheduler</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for callbacks.lr_scheduler</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding: UTF-8</span>
<span class="sd">&quot;&quot;&quot;Learning rate scheduler&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="k">import</span> <span class="n">lr_scheduler</span> <span class="k">as</span> <span class="n">lrs</span>

<span class="kn">from</span> <span class="nn">torchtools.exceptions</span> <span class="k">import</span> <span class="n">CallbackCheckError</span>
<span class="kn">from</span> <span class="nn">torchtools.callbacks.callback</span> <span class="k">import</span> <span class="n">Callback</span>


<span class="k">class</span> <span class="nc">LRScheduler</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_callback_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cb</span><span class="p">,</span> <span class="n">LRScheduler</span><span class="p">):</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;Only one learning rate scheduler should be used&#39;</span>
                <span class="k">raise</span> <span class="n">CallbackCheckError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_StepMixin</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>  <span class="c1"># For PyTorch schedulers</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_StepMixin</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<div class="viewcode-block" id="LambdaLR"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.LambdaLR">[docs]</a><span class="k">class</span> <span class="nc">LambdaLR</span><span class="p">(</span><span class="n">_StepMixin</span><span class="p">,</span> <span class="n">LRScheduler</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Callback that sets the learning rate with a function.</span>

<span class="sd">    Sets the learning rate of each parameter group to the initial lr times</span>
<span class="sd">    a given function.</span>

<span class="sd">    This callback is a wrapper for PyTorch lr_schedulers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="LambdaLR.__init__"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.LambdaLR.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization for LambdaLR.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        optimizer: torch.optim.Optimizer</span>
<span class="sd">            Optimizer for the training net.</span>
<span class="sd">        lr_lambda: function or list</span>
<span class="sd">            A function which computes a multiplicative factor given an integer</span>
<span class="sd">            parameter epoch, or a list of such functions, one for each group</span>
<span class="sd">            in optimizer.param_groups.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LambdaLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">lrs</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="StepLR"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.StepLR">[docs]</a><span class="k">class</span> <span class="nc">StepLR</span><span class="p">(</span><span class="n">_StepMixin</span><span class="p">,</span> <span class="n">LRScheduler</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Callback that sets the learning rate with a decay rate.</span>

<span class="sd">    Sets the learning rate of each parameter group to the initial lr decayed</span>
<span class="sd">    by gamma every step_size epochs.</span>

<span class="sd">    This callback is a wrapper for PyTorch lr_schedulers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="StepLR.__init__"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.StepLR.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization for StepLR.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        optimizer: torch.optim.Optimizer</span>
<span class="sd">            Optimizer for the training net.</span>
<span class="sd">        step_size: int</span>
<span class="sd">            Period of learning rate decay.</span>
<span class="sd">        gamma: float</span>
<span class="sd">            Multiplicative factor of learning rate decay. Default: 0.1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StepLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">lrs</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MultiStepLR"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.MultiStepLR">[docs]</a><span class="k">class</span> <span class="nc">MultiStepLR</span><span class="p">(</span><span class="n">_StepMixin</span><span class="p">,</span> <span class="n">LRScheduler</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Callback that sets the learning rate with epoch milestones.</span>

<span class="sd">    Set the learning rate of each parameter group to the initial lr decayed</span>
<span class="sd">    by gamma once the number of epoch reaches one of the milestones.</span>

<span class="sd">    This callback is a wrapper for PyTorch lr_schedulers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="MultiStepLR.__init__"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.MultiStepLR.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization for MultiStepLR.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        optimizer: torch.optim.Optimizer</span>
<span class="sd">            Optimizer for the training net.</span>
<span class="sd">        milestones: list</span>
<span class="sd">            List of epoch indices. Must be increasing.</span>
<span class="sd">        gamma: float</span>
<span class="sd">            Multiplicative factor of learning rate decay. Default: 0.1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiStepLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">lrs</span><span class="o">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ExponentialLR"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.ExponentialLR">[docs]</a><span class="k">class</span> <span class="nc">ExponentialLR</span><span class="p">(</span><span class="n">_StepMixin</span><span class="p">,</span> <span class="n">LRScheduler</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Callback that sets the learning rate with a decay rate.</span>

<span class="sd">    Set the learning rate of each parameter group to the initial lr decayed</span>
<span class="sd">    by gamma every epoch.</span>

<span class="sd">    This callback is a wrapper for PyTorch lr_schedulers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="ExponentialLR.__init__"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.ExponentialLR.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization for ExponentialLR.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        optimizer: torch.optim.Optimizer</span>
<span class="sd">            Optimizer for the training net.</span>
<span class="sd">        gamma: float</span>
<span class="sd">            Multiplicative factor of learning rate decay. Default: 0.1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">lrs</span><span class="o">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ReduceLROnPlateau"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.ReduceLROnPlateau">[docs]</a><span class="k">class</span> <span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Callback that reduces the learning rate if monitor value stop improving.</span>

<span class="sd">    Reduce learning rate when a metric has stopped improving. Models often</span>
<span class="sd">    benefit from reducing the learning rate by a factor of 2-10 once learning</span>
<span class="sd">    stagnates. This callback reads a monitor value and if no improvement</span>
<span class="sd">    is seen for a `patience` number of epochs, the learning rate is reduced.</span>

<span class="sd">    This callback is a wrapper for PyTorch lr_schedulers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="ReduceLROnPlateau.__init__"><a class="viewcode-back" href="../../callbacks.html#callbacks.lr_scheduler.ReduceLROnPlateau.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">monitor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="s1">&#39;rel&#39;</span><span class="p">,</span> <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">min_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization for ReduceLROnPlateau.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        optimizer: torch.optim.Optimizer</span>
<span class="sd">            Optimizer for the training net.</span>
<span class="sd">        monitor: str</span>
<span class="sd">            Value to be monitored. Default: `val_loss`.</span>
<span class="sd">        mode: str</span>
<span class="sd">            One of `max`, `min`. For `acc` and `val_acc`,</span>
<span class="sd">            mode should be `max`, for `loss` and `val_loss`, mode should be</span>
<span class="sd">            `min`. Default: &#39;min&#39;</span>
<span class="sd">        factor: float</span>
<span class="sd">            Factor by which the learning rate will be reduced.</span>
<span class="sd">            new_lr = lr * factor. Default: 0.1.</span>
<span class="sd">        patience: int</span>
<span class="sd">            Number of epochs with no improvement after which learning rate</span>
<span class="sd">            will be reduced. Default: 10.</span>
<span class="sd">        threshold: float</span>
<span class="sd">            Threshold for measuring the new optimum, to only focus on</span>
<span class="sd">            significant changes. Default: 1e-4.</span>
<span class="sd">        threshold_mode: str</span>
<span class="sd">            One of &#39;rel&#39;, &#39;abs&#39;. In &#39;rel&#39; mode,</span>
<span class="sd">            dynamic_threshold = best * ( 1 + threshold ) in &#39;max&#39; mode or</span>
<span class="sd">            best * ( 1 - threshold ) in &#39;min&#39; mode. In abs mode,</span>
<span class="sd">            dynamic_threshold = best + threshold in max mode or</span>
<span class="sd">            best - threshold in min mode. Default: &#39;rel&#39;,</span>
<span class="sd">        cooldown: int</span>
<span class="sd">            Number of epochs to wait before resuming normal operation after</span>
<span class="sd">            lr has been reduced. Default: 0.</span>
<span class="sd">        min_lr: float or list</span>
<span class="sd">             A scalar or a list of scalars. A lower bound on the learning rate</span>
<span class="sd">             of all param groups or each group respectively. Default: 0.</span>
<span class="sd">        eps: float</span>
<span class="sd">             Minimal decay applied to lr. If the difference between new and</span>
<span class="sd">             old lr is smaller than eps, the update is ignored. Default: 1e-8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceLROnPlateau</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">lr_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="n">mode</span><span class="p">,</span>
            <span class="s1">&#39;factor&#39;</span><span class="p">:</span> <span class="n">factor</span><span class="p">,</span>
            <span class="s1">&#39;patience&#39;</span><span class="p">:</span> <span class="n">patience</span><span class="p">,</span>
            <span class="s1">&#39;threshold&#39;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">,</span>
            <span class="s1">&#39;threshold_mode&#39;</span><span class="p">:</span> <span class="n">threshold_mode</span><span class="p">,</span>
            <span class="s1">&#39;cooldown&#39;</span><span class="p">:</span> <span class="n">cooldown</span><span class="p">,</span>
            <span class="s1">&#39;min_lr&#39;</span><span class="p">:</span> <span class="n">min_lr</span><span class="p">,</span>
            <span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">eps</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">lrs</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">lr_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">monitor</span></div>

    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;meters&#39;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">val</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Youchen Du.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.1.3',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../static/jquery.js"></script>
      <script type="text/javascript" src="../../static/underscore.js"></script>
      <script type="text/javascript" src="../../static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script> 

</body>
</html>